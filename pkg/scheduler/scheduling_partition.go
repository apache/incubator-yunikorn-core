/*
Copyright 2019 Cloudera, Inc.  All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package scheduler

import (
    "fmt"
    "github.com/cloudera/yunikorn-core/pkg/cache"
    "github.com/cloudera/yunikorn-core/pkg/log"
    "github.com/cloudera/yunikorn-core/pkg/scheduler/placement"
    "go.uber.org/zap"
    "sync"
)

type PartitionSchedulingContext struct {
    Root *SchedulingQueue // start of the scheduling queue hierarchy
    RmId string           // the RM the partition belongs to
    Name string           // name of the partition (logging mainly)

    // Private fields need protection
    partition        *cache.PartitionInfo              // link back to the partition in the cache
    applications     map[string]*SchedulingApplication // applications assigned to this partition
    queues           map[string]*SchedulingQueue       // scheduling queues flattened not a hierarchy
    placementManager *placement.Manager                // placement manager for this partition
    partitionManager *PartitionManager                 // manager for this partition
    lock             sync.RWMutex
}

// Create a new partitioning scheduling context.
// the flattened list is generated by a separate call
func newPartitionSchedulingContext(info *cache.PartitionInfo, root *SchedulingQueue) *PartitionSchedulingContext {
    psc := &PartitionSchedulingContext{
        applications: make(map[string]*SchedulingApplication),
        queues:       make(map[string]*SchedulingQueue),
        Root:         root,
        Name:         info.Name,
        RmId:         info.RMId,
        partition:    info,
    }
    psc.placementManager = placement.NewPlacementManager(info)
    return psc
}

// Update the scheduling partition based on the reloaded config.
func (psc *PartitionSchedulingContext) updatePartitionSchedulingContext(info *cache.PartitionInfo) {
    psc.lock.Lock()
    defer psc.lock.Unlock()

    if psc.placementManager.IsInitialised() {
        log.Logger.Info("Updating placement manager rules on config reload")
        err := psc.placementManager.UpdateRules(info.GetRules())
        if err != nil {
            log.Logger.Info("New placement rules not activated, config reload failed", zap.Error(err))
        }
    } else {
        log.Logger.Info("Creating new placement manager on config reload")
        psc.placementManager = placement.NewPlacementManager(info)
    }
    root := psc.Root
    // update the root queue properties
    root.updateSchedulingQueueProperties(info.Root.Properties)
    // update the rest of the queues recursively
    root.updateSchedulingQueueInfo(info.Root.GetCopyOfChildren(), root)
}

// Add a new application to the scheduling partition.
func (psc *PartitionSchedulingContext) AddSchedulingApplication(schedulingApp *SchedulingApplication) error {
    psc.lock.Lock()
    defer psc.lock.Unlock()

    // Add to applications
    appId := schedulingApp.ApplicationInfo.ApplicationId
    if psc.applications[appId] != nil {
        return fmt.Errorf("adding application %s to partition %s, but application already existed", appId, psc.Name)
    }

    psc.applications[appId] = schedulingApp

    // Put app under queue
    // TODO use placement rules in all cases i.e. init placement manager with provided rule always
    queueName := schedulingApp.ApplicationInfo.QueueName
    if psc.placementManager.IsInitialised() {
        var err error
        queueName, err = psc.placementManager.PlaceApplication(schedulingApp.ApplicationInfo)
        if err != nil {
            return fmt.Errorf("failed to find queue %s for application %s: %v", schedulingApp.ApplicationInfo.QueueName, appId, err)
        }
    }
    // find the scheduling queue
    schedulingQueue := psc.queues[queueName]
    if schedulingQueue == nil {
        return fmt.Errorf("failed to find queue %s for application %s", schedulingApp.ApplicationInfo.QueueName, appId)
    }
    schedulingApp.queue = schedulingQueue
    schedulingQueue.AddSchedulingApplication(schedulingApp)

    return nil
}

// Remove the application from the scheduling partition.
func (psc *PartitionSchedulingContext) RemoveSchedulingApplication(appId string) (*SchedulingApplication, error) {
    psc.lock.Lock()
    defer psc.lock.Unlock()

    // Remove from applications map
    if psc.applications[appId] == nil {
        return nil, fmt.Errorf("removing application %s from partition %s, but application does not exist", appId, psc.Name)
    }
    schedulingApp := psc.applications[appId]
    delete(psc.applications, appId)

    // Remove app under queue
    schedulingQueue := psc.queues[schedulingApp.ApplicationInfo.QueueName]
    if schedulingQueue == nil {
        // This is not normal
        panic(fmt.Sprintf("Failed to find queue %s for app=%s while removing application", schedulingApp.ApplicationInfo.QueueName, appId))
    }
    schedulingQueue.RemoveSchedulingApplication(schedulingApp)

    return schedulingApp, nil
}

// Visible by tests
// TODO need to think about this: maintaining the flat map might not scale with dynamic queues
func (psc *PartitionSchedulingContext) GetQueue(queueName string) *SchedulingQueue {
    psc.lock.RLock()
    defer psc.lock.RUnlock()

    return psc.queues[queueName]
}

// Remove the queue from the flat map.
func (psc *PartitionSchedulingContext) RemoveQueue(queueName string) {
    psc.lock.Lock()
    defer psc.lock.Unlock()

    delete(psc.queues, queueName)
}

func (psc *PartitionSchedulingContext) getApplication(appId string) *SchedulingApplication {
    psc.lock.RLock()
    defer psc.lock.RUnlock()

    return psc.applications[appId]
}
